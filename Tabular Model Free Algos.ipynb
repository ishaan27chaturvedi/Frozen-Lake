{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71348c7f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from itertools import product\n",
    "import contextlib\n",
    "from Environment import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b97399f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures numpy print options\n",
    "@contextlib.contextmanager\n",
    "def _printoptions(*args, **kwargs):\n",
    "    original = np.get_printoptions()\n",
    "    np.set_printoptions(*args, **kwargs)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        np.set_printoptions(**original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2fc553",
   "metadata": {},
   "source": [
    "# Tabular Model Free Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab0cbc",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f3aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_coeff(env, max_episodes, eta, epsilon, seed):\n",
    "    \"\"\"\n",
    "    Define random seed, eta (decaying learning rate), \n",
    "    epsilon (decaying exploration rate) and Q values (for state action pairs)\n",
    "    \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    eta = np.linspace(eta, 0, max_episodes) \n",
    "    epsilon = np.linspace(epsilon, 0, max_episodes) \n",
    "    q = np.zeros((env.n_states, env.n_actions))\n",
    "    \n",
    "    return q, epsilon, eta, random_state\n",
    "\n",
    "\n",
    "def randomBestAction(random_state, mean_rewards):\n",
    "    \"\"\"\n",
    "    Get an array of best actions based on Q values (mean_rewards)\n",
    "    Break ties randomly and return one of the best actions\n",
    "    \"\"\"\n",
    "    best_actions = np.array(np.argwhere(mean_rewards == np.amax(mean_rewards))).flatten()\n",
    "    \n",
    "    return random_state.choice(best_actions, 1)[0]  \n",
    "\n",
    "\n",
    "def select_action(env, q, random_state, epsilon, i):\n",
    "    \"\"\"\n",
    "    Select action a for state s according to an e-greedy policy based on Q values.\n",
    "    Use Epsilon Greedy method to decide whether to take best action or random action.\n",
    "    \"\"\"\n",
    "    \n",
    "    if(random_state.random(1) < epsilon[i]):\n",
    "        a = random_state.choice(range(env.n_actions)) \n",
    "    else:\n",
    "        a = randomBestAction(random_state, q)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e6842",
   "metadata": {},
   "source": [
    "# SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82a4bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
    "    \"\"\"\n",
    "    Initialize coefficients, Q values and random state.\n",
    "    Iterating through max episodes, start game and choose an action.\n",
    "    Until game over, play action to get rewards and new state. Use new state to select new action.\n",
    "    Update Q values based on states, actions and coefficients.\n",
    "    When all iterations are over, return optimal values and policy.\n",
    "    \"\"\"\n",
    "    q, epsilon, eta, random_state = initialize_coeff(env, max_episodes, eta, epsilon, seed)\n",
    "    \n",
    "    for i in range(max_episodes):   \n",
    "        s, done = env.reset(), False          \n",
    "        a = select_action(env, q[s], random_state, epsilon, i)\n",
    "        \n",
    "        while(not done):\n",
    "            s_new, r, done = env.step(a)\n",
    "            a_new = select_action(env, q[s_new], random_state, epsilon, i)\n",
    "            q[s,a] += eta[i] * (r + gamma * q[s_new, a_new] - q[s,a])\n",
    "            s, a = s_new, a_new\n",
    "\n",
    "    policy = q.argmax(axis=1)\n",
    "    value = q.max(axis=1)\n",
    "\n",
    "    return policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3bcb5",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12c619bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
    "    \"\"\"\n",
    "    Initialize coefficients, Q values and random state.\n",
    "    Iterating through max episodes: start game.\n",
    "    Until game over, choose action, play action to get rewards and new state. \n",
    "    Use new state to select best new action.\n",
    "    Update Q values based on states, actions and coefficients.\n",
    "    When all iterations are over, return optimal values and policy.\n",
    "    \"\"\"\n",
    "    q, epsilon, eta, random_state = initialize_coeff(env, max_episodes, eta, epsilon, seed)\n",
    "\n",
    "    for i in range(max_episodes):\n",
    "        s, done = env.reset(), False\n",
    "        \n",
    "        while(not done):\n",
    "            a = select_action(env, q[s], random_state, epsilon, i)\n",
    "            s_new, r, done = env.step(a)\n",
    "            q_max = max(q[s_new])\n",
    "            q[s,a] += eta[i] * (r + gamma * q_max - q[s,a])\n",
    "            s = s_new\n",
    "            \n",
    "    policy = q.argmax(axis=1)\n",
    "    value = q.max(axis=1)\n",
    "\n",
    "    return policy, value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0dba3",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be4763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 20000\n",
    "eta = 0.5\n",
    "epsilon = 0.5\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc60d6",
   "metadata": {},
   "source": [
    "## SARSA run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7356534f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake:\n",
      "[['&' '.' '.' '.']\n",
      " ['.' '#' '.' '#']\n",
      " ['.' '.' '.' '#']\n",
      " ['#' '.' '.' '$']]\n",
      "Policy:\n",
      "[['→' '→' '↓' '←']\n",
      " ['↓' '↑' '↓' '↑']\n",
      " ['→' '↓' '↓' '↑']\n",
      " ['↑' '→' '→' '↑']]\n",
      "Value:\n",
      "[[0.44  0.492 0.571 0.482]\n",
      " [0.449 0.    0.655 0.   ]\n",
      " [0.533 0.649 0.779 0.   ]\n",
      " [0.    0.76  0.889 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "policy, value = sarsa(env, max_episodes, eta, gamma, epsilon, seed=seed)\n",
    "env.render(policy, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7fe98",
   "metadata": {},
   "source": [
    "## Q-Learning Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5beabb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake:\n",
      "[['&' '.' '.' '.']\n",
      " ['.' '#' '.' '#']\n",
      " ['.' '.' '.' '#']\n",
      " ['#' '.' '.' '$']]\n",
      "Policy:\n",
      "[['↓' '→' '↓' '←']\n",
      " ['↓' '↑' '↓' '↑']\n",
      " ['→' '↓' '↓' '↑']\n",
      " ['↑' '→' '→' '↑']]\n",
      "Value:\n",
      "[[0.458 0.458 0.558 0.512]\n",
      " [0.511 0.    0.627 0.   ]\n",
      " [0.594 0.678 0.755 0.   ]\n",
      " [0.    0.777 0.889 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "policy, value = q_learning(env, max_episodes, eta, gamma, epsilon, seed=seed)\n",
    "env.render(policy, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd85e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
