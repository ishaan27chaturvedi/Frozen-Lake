{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d35ef2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from itertools import product\n",
    "import contextlib\n",
    "from Environment import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "136e8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configures numpy print options\n",
    "@contextlib.contextmanager\n",
    "def _printoptions(*args, **kwargs):\n",
    "    original = np.get_printoptions()\n",
    "    np.set_printoptions(*args, **kwargs)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        np.set_printoptions(**original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6990b74a",
   "metadata": {},
   "source": [
    "# Non Tabular Model Free Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97112b3e",
   "metadata": {},
   "source": [
    "## Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71fa88e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWrapper:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "        self.n_actions = self.env.n_actions\n",
    "        self.n_states = self.env.n_states\n",
    "        self.n_features = self.n_actions * self.n_states\n",
    "        self.absorbing_state = self.env.absorbing_state\n",
    "\n",
    "    def encode_state(self, s):\n",
    "        features = np.zeros((self.n_actions, self.n_features))\n",
    "        for a in range(self.n_actions):\n",
    "            i = np.ravel_multi_index((s, a), (self.n_states, self.n_actions))\n",
    "            features[a, i] = 1.0\n",
    "\n",
    "        return features\n",
    "\n",
    "    def decode_policy(self, theta):\n",
    "        policy = np.zeros(self.env.n_states, dtype=int)\n",
    "        value = np.zeros(self.env.n_states)\n",
    "\n",
    "        for s in range(self.n_states):\n",
    "            features = self.encode_state(s)\n",
    "            q = features.dot(theta)\n",
    "\n",
    "            policy[s] = np.argmax(q)\n",
    "            value[s] = np.max(q)\n",
    "\n",
    "        return policy, value\n",
    "\n",
    "    def reset(self):\n",
    "        return self.encode_state(self.env.reset())\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done = self.env.step(action)\n",
    "\n",
    "        return self.encode_state(state), reward, done\n",
    "\n",
    "    def render(self, policy=None, value=None):\n",
    "        self.env.render(policy, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe07ba",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "090710a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_coeff(env, max_episodes, eta, epsilon, seed):\n",
    "    \"\"\"\n",
    "    Define random seed, eta (decaying learning rate), \n",
    "    epsilon (decaying exploration rate) and Q values (for state action pairs)\n",
    "    \"\"\"\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    eta = np.linspace(eta, 0, max_episodes) \n",
    "    epsilon = np.linspace(epsilon, 0, max_episodes) \n",
    "    theta = np.zeros(env.n_features) \n",
    "    \n",
    "    return theta, epsilon, eta, random_state\n",
    "\n",
    "\n",
    "def randomBestAction(random_state, mean_rewards):\n",
    "    \"\"\"\n",
    "    Get an array of best actions based on Q values (mean_rewards)\n",
    "    Break ties randomly and return one of the best actions\n",
    "    \"\"\"\n",
    "    best_actions = np.array(np.argwhere(mean_rewards == np.amax(mean_rewards))).flatten()\n",
    "    \n",
    "    return random_state.choice(best_actions, 1)[0]  \n",
    "\n",
    "\n",
    "def select_action(env, q, random_state, epsilon, i):\n",
    "    \"\"\"\n",
    "    Select action a for state s according to an e-greedy policy based on Q values.\n",
    "    Use Epsilon Greedy method to decide whether to take best action or random action.\n",
    "    \"\"\"\n",
    "    \n",
    "    if(random_state.random(1) < epsilon[i]):\n",
    "        a = random_state.choice(range(env.n_actions)) \n",
    "    else:\n",
    "        a = randomBestAction(random_state, q)\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1351df5",
   "metadata": {},
   "source": [
    "## Linear SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c33023db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_sarsa(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
    "    \"\"\"\n",
    "    Initialize coefficients, theta values and random state.\n",
    "    Iterating through max episodes, start game, define q and choose an action.\n",
    "    Until game over, play action to get rewards and new features. \n",
    "    Use new features to update new q values and theta values. Then select new action.\n",
    "    Update Q values, delta and theta based on features, rewards and coefficients.\n",
    "    When all iterations are over, return optimal values and policy.\n",
    "    \"\"\"\n",
    "    theta, epsilon, eta, random_state = initialize_coeff(env, max_episodes, eta, epsilon, seed)\n",
    "    \n",
    "    for i in range(max_episodes):\n",
    "        features, done = env.reset(), False\n",
    "        q = features.dot(theta)\n",
    "        a = select_action(env, q, random_state, epsilon, i)\n",
    "\n",
    "        while not done:\n",
    "            features_prime, r, done = env.step(a)\n",
    "            delta = r - q[a]\n",
    "            q = features_prime.dot(theta)  \n",
    "            a_prime = select_action(env, q, random_state, epsilon, i)\n",
    "\n",
    "            delta += (gamma * q[a_prime])\n",
    "            theta += eta[i] * delta * features[a]\n",
    "            features = features_prime\n",
    "            a = a_prime\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed8f37",
   "metadata": {},
   "source": [
    "## Linear Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f00daa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_q_learning(env, max_episodes, eta, gamma, epsilon, seed=None):\n",
    "    \"\"\"\n",
    "    Initialize coefficients, Q values and random state.\n",
    "    Iterating through max episodes: start game.\n",
    "    Until game over, choose action, play action to get rewards and new features. \n",
    "    Use new features to select best new action.\n",
    "    Update Q values based on features, theta, actions and coefficients.\n",
    "    When all iterations are over, return optimal values and policy.\n",
    "    \"\"\"\n",
    "    theta, epsilon, eta, random_state = initialize_coeff(env, max_episodes, eta, epsilon, seed)\n",
    "\n",
    "    for i in range(max_episodes):\n",
    "        features, done = env.reset(), False\n",
    "        q = features.dot(theta)\n",
    "\n",
    "        while not done:\n",
    "            a = select_action(env, q, random_state, epsilon, i)\n",
    "            features_prime, r, done = env.step(a)\n",
    "            delta = r - q[a]\n",
    "            q = features_prime.dot(theta)\n",
    "            \n",
    "            delta += (gamma * max(q))\n",
    "            theta += eta[i] * delta * features[a]\n",
    "            features = features_prime\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311c4c6",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f03c91a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_env = LinearWrapper(env)\n",
    "\n",
    "max_episodes = 20000\n",
    "eta = 0.5\n",
    "epsilon = 0.5\n",
    "gamma = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9b1fc",
   "metadata": {},
   "source": [
    "## Run Linear SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa14ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake:\n",
      "[['&' '.' '.' '.']\n",
      " ['.' '#' '.' '#']\n",
      " ['.' '.' '.' '#']\n",
      " ['#' '.' '.' '$']]\n",
      "Policy:\n",
      "[['↓' '→' '↓' '←']\n",
      " ['↓' '↑' '↓' '↑']\n",
      " ['→' '↓' '↓' '↑']\n",
      " ['↑' '→' '→' '↑']]\n",
      "Value:\n",
      "[[0.436 0.458 0.521 0.438]\n",
      " [0.495 0.    0.602 0.   ]\n",
      " [0.574 0.664 0.77  0.   ]\n",
      " [0.    0.77  0.886 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "parameters = linear_sarsa(linear_env, max_episodes, eta, gamma, epsilon, seed=seed)\n",
    "policy, value = linear_env.decode_policy(parameters)\n",
    "linear_env.render(policy, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c896dcde",
   "metadata": {},
   "source": [
    "## Run Linear Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54352911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lake:\n",
      "[['&' '.' '.' '.']\n",
      " ['.' '#' '.' '#']\n",
      " ['.' '.' '.' '#']\n",
      " ['#' '.' '.' '$']]\n",
      "Policy:\n",
      "[['→' '→' '↓' '←']\n",
      " ['↓' '↑' '↓' '↑']\n",
      " ['→' '→' '↓' '↑']\n",
      " ['↑' '→' '→' '↑']]\n",
      "Value:\n",
      "[[0.456 0.511 0.583 0.505]\n",
      " [0.459 0.    0.656 0.   ]\n",
      " [0.548 0.68  0.771 0.   ]\n",
      " [0.    0.766 0.885 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "parameters = linear_q_learning(linear_env, max_episodes, eta, gamma, epsilon, seed=seed)\n",
    "policy, value = linear_env.decode_policy(parameters)\n",
    "linear_env.render(policy, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4191751d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
